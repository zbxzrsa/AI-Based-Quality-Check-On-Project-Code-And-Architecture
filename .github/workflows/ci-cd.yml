name: CI/CD Pipeline

on:
    push:
        branches: [main, develop]
    pull_request:
        branches: [main, develop]

jobs:
    backend-tests:
        name: Backend Tests
        runs-on: ubuntu-latest

        services:
            postgres:
                image: postgres:15
                env:
                    POSTGRES_USER: postgres
                    POSTGRES_PASSWORD: postgres
                    POSTGRES_DB: ai_code_review_test
                ports:
                    - 5432:5432
                options: >-
                    --health-cmd pg_isready
                    --health-interval 10s
                    --health-timeout 5s
                    --health-retries 5

            redis:
                image: redis:7-alpine
                ports:
                    - 6379:6379
                options: >-
                    --health-cmd "redis-cli ping"
                    --health-interval 10s
                    --health-timeout 5s
                    --health-retries 5

            neo4j:
                image: neo4j:5
                env:
                    NEO4J_AUTH: neo4j/testpassword
                ports:
                    - 7687:7687
                    - 7474:7474

        steps:
            - uses: actions/checkout@v6

            - name: Set up Python
              uses: actions/setup-python@v4
              with:
                  python-version: "3.11"
                  cache: "pip"

            - name: Install dependencies
              run: |
                  cd backend
                  pip install -r requirements.txt
                  pip install pytest pytest-cov pytest-asyncio

            - name: Run database migrations
              env:
                  POSTGRES_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/ai_code_review_test
              run: |
                  cd backend
                  alembic upgrade head

            - name: Run tests with coverage
              env:
                  POSTGRES_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/ai_code_review_test
                  REDIS_URL: redis://localhost:6379
                  NEO4J_URI: bolt://localhost:7687
                  NEO4J_USER: neo4j
                  NEO4J_PASSWORD: testpassword
                  JWT_SECRET: test-secret-key
                  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
                  TEST_RESULTS_JSON: test-results-ai-reviewer.json
              run: |
                  cd backend
                  pytest tests/ -v --cov=app --cov-report=xml --cov-report=html
              continue-on-error: true

            - name: Analyze Test Failures
              if: failure()
              run: |
                  cd backend
                  python -c "
                  import json
                  try:
                      with open('test-results-ai-reviewer.json', 'r') as f:
                          results = json.load(f)
                          print('ðŸ§ª Test Failure Analysis for AI Reviewer:')
                          print(f'  Test Run ID: {results.get(\"test_run_id\", \"unknown\")}')
                          print(f'  Total Tests: {results.get(\"summary\", {}).get(\"total_tests\", 0)}')
                          print(f'  Failures: {len(results.get(\"failures\", []))}')
                          print(f'  Errors: {len(results.get(\"errors\", []))}')

                          # Analyze common failure patterns
                          neo4j_failures = [f for f in results.get('failures', []) + results.get('errors', []) if 'neo4j' in f.get('exception_message', '').lower() or 'neo4j' in f.get('traceback', '').lower()]
                          async_failures = [f for f in results.get('failures', []) + results.get('errors', []) if 'async' in f.get('exception_message', '').lower() or 'event loop' in f.get('exception_message', '').lower()]
                          validation_failures = [f for f in results.get('failures', []) + results.get('errors', []) if 'validation' in f.get('exception_message', '').lower() or 'pydantic' in f.get('exception_message', '').lower()]

                          print(f'  Neo4j-related failures: {len(neo4j_failures)}')
                          print(f'  Async/event loop failures: {len(async_failures)}')
                          print(f'  Pydantic validation failures: {len(validation_failures)}')

                          if neo4j_failures:
                              print('  âš ï¸  Possible Neo4j connection issues detected')
                          if async_failures:
                              print('  âš ï¸  Possible async event loop issues detected')
                          if validation_failures:
                              print('  âš ï¸  Possible Pydantic validation issues detected')

                  except FileNotFoundError:
                      print('âŒ Test results JSON not found')
                  except json.JSONDecodeError as e:
                      print(f'âŒ Error parsing test results JSON: {e}')
                  "

            - name: Upload coverage reports
              uses: codecov/codecov-action@v3
              with:
                  file: ./backend/coverage.xml
                  flags: backend

            - name: Generate Test Results Summary
              if: always()
              run: |
                  echo "## ðŸ§ª Backend Test Results Summary" > backend-test-summary.md
                  echo "" >> backend-test-summary.md
                  echo "**Test Run:** ${{ github.run_number }}" >> backend-test-summary.md
                  echo "**Commit:** ${{ github.sha }}" >> backend-test-summary.md
                  echo "**Date:** $(date -u)" >> backend-test-summary.md
                  echo "" >> backend-test-summary.md
                  
                  if [ -f test-results-ai-reviewer.json ]; then
                      echo "### Test Statistics" >> backend-test-summary.md
                      echo "- **Total Tests:** $(jq '.summary.total_tests // 0' test-results-ai-reviewer.json)" >> backend-test-summary.md
                      echo "- **Passed:** $(jq '.summary.passed // 0' test-results-ai-reviewer.json)" >> backend-test-summary.md
                      echo "- **Failed:** $(jq '.summary.failed // 0' test-results-ai-reviewer.json)" >> backend-test-summary.md
                      echo "- **Errors:** $(jq '.summary.errors // 0' test-results-ai-reviewer.json)" >> backend-test-summary.md
                      echo "- **Skipped:** $(jq '.summary.skipped // 0' test-results-ai-reviewer.json)" >> backend-test-summary.md
                      echo "" >> backend-test-summary.md
                      
                      # Count specific types of failures
                      neo4j_failures=$(jq '.failures | length + .errors | length' test-results-ai-reviewer.json)
                      echo "- **Neo4j-related issues:** $neo4j_failures" >> backend-test-summary.md
                  else
                      echo "### Test Statistics" >> backend-test-summary.md
                      echo "- **Status:** Test results file not found" >> backend-test-summary.md
                  fi
                  
                  echo "" >> backend-test-summary.md
                  echo "### Coverage Report" >> backend-test-summary.md
                  echo "- **Coverage:** See uploaded coverage artifacts" >> backend-test-summary.md
                  echo "- **Detailed Report:** Available in coverage artifacts" >> backend-test-summary.md
                  echo "" >> backend-test-summary.md
                  
                  echo "---" >> backend-test-summary.md
                  echo "*Generated by Backend Tests workflow*" >> backend-test-summary.md
                  
                  cat backend-test-summary.md

            - name: Upload Test Summary
              if: always()
              uses: actions/upload-artifact@v6
              with:
                  name: backend-test-summary
                  path: backend-test-summary.md

            - name: AI Self-Healing Analysis
              if: failure() && github.event_name == 'pull_request'
              run: |
                  echo "ðŸ¤– Running AI Self-Healing Analysis..."
                  cd backend
                  python ../scripts/ai_self_healing.py --analyze-failure --pr-number ${{ github.event.number }}
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
                  OLLAMA_URL: ${{ secrets.OLLAMA_URL }}
                  OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL }}
              continue-on-error: true

    frontend-tests:
        name: Frontend Tests
        runs-on: ubuntu-latest

        steps:
            - uses: actions/checkout@v6

            - name: Set up Node.js
              uses: actions/setup-node@v4
              with:
                  node-version: "20"
                  cache: "npm"
                  cache-dependency-path: frontend/package-lock.json

            - name: Configure environment variables
              run: |
                  # Create .env.local for Next.js build and tests
                  cat > frontend/.env.local << EOF
                  NEXT_PUBLIC_API_URL=http://localhost:8000/api/v1
                  NEXT_PUBLIC_APP_ENV=test
                  NEXT_PUBLIC_GITHUB_CLIENT_ID=\${{ secrets.NEXT_PUBLIC_GITHUB_CLIENT_ID }}
                  NEXT_PUBLIC_ANALYTICS_ID=\${{ secrets.NEXT_PUBLIC_ANALYTICS_ID }}
                  # Server-side variables for Next.js
                  JWT_SECRET=test-jwt-secret-for-ci
                  GITHUB_TOKEN=\${{ secrets.GITHUB_TOKEN }}
                  GITHUB_WEBHOOK_SECRET=\${{ secrets.GITHUB_WEBHOOK_SECRET }}
                  EOF

            - name: Install dependencies
              run: |
                  cd frontend
                  # Use exact versions and resolve peer dependency conflicts
                  npm ci --prefer-offline --no-audit --legacy-peer-deps
                  
                  # Verify installation
                  npm ls --depth=0
                  
                  # Check for any remaining peer dependency issues
                  echo "ðŸ” Checking for peer dependency issues..."
                  npm install --dry-run --legacy-peer-deps || echo "âš ï¸  Some peer dependency warnings are expected"

            - name: Verify environment setup
              run: |
                  bash ../scripts/verify-frontend-env-enhanced.sh

            - name: Install Playwright browsers for E2E tests
              run: |
                  cd frontend
                  npx playwright install --with-deps chromium
              continue-on-error: true

            - name: Run linting
              run: |
                  cd frontend
                  npm run lint

            - name: Run type checking
              run: |
                  cd frontend
                  npm run type-check

            - name: Run unit tests with Headless Chrome
              run: |
                  cd frontend
                  npm test -- --coverage --watchAll=false --maxWorkers=2
              env:
                  NODE_OPTIONS: "--max-old-space-size=4096"
                  CI: true
                  JEST_MAX_WORKERS: 2

            - name: Build application
              run: |
                  cd frontend
                  npm run build
              env:
                  NODE_OPTIONS: "--max-old-space-size=4096"

            - name: Upload coverage reports
              uses: actions/upload-artifact@v6
              with:
                  file: ./frontend/coverage/coverage-final.json
                  flags: frontend

            - name: Upload test results
              if: always()
              uses: actions/upload-artifact@v6
              with:
                  name: frontend-test-results
                  path: frontend/junit.xml
              continue-on-error: true

    e2e-tests:
        name: End-to-End Tests
        runs-on: ubuntu-latest
        needs: [backend-tests, frontend-tests]

        steps:
            - uses: actions/checkout@v6

            - name: Set up Python
              uses: actions/setup-python@v4
              with:
                  python-version: "3.11"

            - name: Set up Node.js
              uses: actions/setup-node@v4
              with:
                  node-version: "18"

            - name: Install Playwright
              run: |
                  cd frontend
                  npm ci
                  npx playwright install --with-deps

            - name: Start services
              run: |
                  docker-compose up -d
                  sleep 30

            - name: Run E2E tests
              run: |
                  cd frontend
                  npx playwright test

            - name: Upload test results
              if: always()
              uses: actions/upload-artifact@v6
              with:
                  name: playwright-report
                  path: frontend/playwright-report/

    deploy:
        name: Deploy to Production
        runs-on: ubuntu-latest
        needs: [backend-tests, frontend-tests, e2e-tests]
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'

        steps:
            - uses: actions/checkout@v6

            - name: Configure AWS credentials
              uses: aws-actions/configure-aws-credentials@v5
              with:
                  aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
                  aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
                  aws-region: us-east-1

            - name: Deploy to AWS
              run: |
                  # Add deployment commands here
                  echo "Deploying to production..."
