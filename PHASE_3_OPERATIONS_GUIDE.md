# Phase 3: ç³»ç»Ÿä¼˜åŒ–ä¸è¿ç»´ - å®Œæ•´è¿ä½œæŒ‡å—

## ç›®å½•

1. [å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ— (Redis + Celery)](#å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—)
2. [æ¶æ„æ¼‚ç§»æ£€æµ‹æŸ¥è¯¢](#æ¶æ„æ¼‚ç§»æ£€æµ‹)
3. [Docker Compose é…ç½®](#docker-compose-é…ç½®)
4. [è¿è¡Œä¸éƒ¨ç½²](#è¿è¡Œä¸éƒ¨ç½²)
5. [ç›‘æ§ä¸è°ƒè¯•](#ç›‘æ§ä¸è°ƒè¯•)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

### æ¶æ„æ¦‚è¿°

```
GitHub Webhook
     â†“
FastAPI Endpoint (ç«‹å³å“åº”)
     â†“
Celery Task Queue (Redis)
     â†“
åˆ†ä¸ºä¸¤ä¸ª Worker:
  - High Priority: PR åˆ†æ (å¹¶å‘: 2)
  - Low Priority: æ¶æ„æ¼‚ç§»æ£€æµ‹ (å¹¶å‘: 1)
     â†“
å­˜å‚¨ç»“æœ (PostgreSQL)
```

### å…³é”®ç‰¹æ€§

âœ… **éé˜»å¡ API**: ç«¯ç‚¹ç«‹å³è¿”å› task_idï¼Œæ— éœ€ç­‰å¾…åˆ†æå®Œæˆ
âœ… **ä¼˜å…ˆçº§é˜Ÿåˆ—**: PR åˆ†æä¼˜å…ˆçº§é«˜ï¼Œæ¼‚ç§»æ£€æµ‹ä¼˜å…ˆçº§ä½
âœ… **è‡ªåŠ¨é‡è¯•**: å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š 3 æ¬¡
âœ… **ä»»åŠ¡è¿½è¸ª**: é€šè¿‡ task_id è½®è¯¢ä»»åŠ¡çŠ¶æ€
âœ… **å®šæ—¶è°ƒåº¦**: ä½¿ç”¨ Celery Beat å®ç°å‘¨æœŸæ€§ä»»åŠ¡

### API ç«¯ç‚¹

#### 1. é˜Ÿåˆ— PR åˆ†æ

```bash
POST /api/v1/analysis/projects/{project_id}/analyze?pr_id={pr_id}

Response (ç«‹å³è¿”å›):
{
  "task_id": "abc123xyz789",
  "status": "PENDING",
  "pr_id": "pr-1",
  "message": "PR analysis queued and will begin shortly"
}

å“åº”æ—¶é—´: < 50ms âœ“
```

#### 2. æ£€æŸ¥ä»»åŠ¡çŠ¶æ€

```bash
GET /api/v1/analysis/{task_id}/status

Response (PENDING):
{
  "task_id": "abc123xyz789",
  "status": "PENDING",
  "result": null,
  "error": null
}

Response (SUCCESS):
{
  "task_id": "abc123xyz789",
  "status": "SUCCESS",
  "result": {
    "pr_id": "pr-1",
    "status": "completed",
    "issues_found": 5,
    "risk_score": 45.5,
    "confidence_score": 0.92
  },
  "error": null
}

Response (FAILURE):
{
  "task_id": "abc123xyz789",
  "status": "FAILURE",
  "result": null,
  "error": "Connection timeout"
}
```

#### 3. é‡æ–°åˆ†æ PR

```bash
POST /api/v1/analysis/projects/{project_id}/pull-requests/{pr_id}/reanalyze

Response:
{
  "message": "PR re-analysis queued",
  "task_id": "new_task_id",
  "status": "PENDING",
  "pr_id": "pr-1"
}
```

### Celery ä»»åŠ¡å®šä¹‰

#### analyze_pull_request (é«˜ä¼˜å…ˆçº§)

```python
@celery_app.task(
    bind=True,
    name='app.tasks.analyze_pull_request',
    max_retries=3,
    queue='high_priority'
)
def analyze_pull_request(self, pr_id: str, project_id: str):
    """
    æ‰§è¡Œæ­¥éª¤:
    1. ä» GitHub è·å– PR æ–‡ä»¶å’Œå·®å¼‚
    2. ä½¿ç”¨ AST è§£æå™¨è§£æå˜æ›´æ–‡ä»¶
    3. åœ¨ Neo4j ä¸­æ„å»ºä¾èµ–å›¾
    4. è¿è¡Œ AI æ¨ç†å¼•æ“è¿›è¡Œåˆ†æ
    5. å­˜å‚¨ç»“æœåˆ° PostgreSQL
    6. æ›´æ–° GitHub PR æ£€æŸ¥çŠ¶æ€
    """
```

#### detect_architectural_drift (ä½ä¼˜å…ˆçº§)

```python
@celery_app.task(
    name='app.tasks.detect_architectural_drift',
    max_retries=2,
    queue='low_priority'
)
def detect_architectural_drift(project_id: str, baseline_version: str = "latest"):
    """
    æ‰§è¡Œæ­¥éª¤:
    1. æ£€æµ‹å¾ªç¯ä¾èµ–
    2. æ£€æµ‹å±‚è¿è§„
    3. è®¡ç®—è€¦åˆæŒ‡æ ‡
    4. ç”Ÿæˆæ¼‚ç§»æŠ¥å‘Š
    5. å­˜å‚¨åˆ°æ•°æ®åº“
    """
```

---

## æ¶æ„æ¼‚ç§»æ£€æµ‹

### Cypher æŸ¥è¯¢é›†åˆ

#### 1. æ£€æµ‹å¾ªç¯ä¾èµ–

**æŸ¥è¯¢** (è§ cypher_queries.py):

```cypher
CYCLIC_DEPENDENCY_QUERY

æ‰¾åˆ°çš„æ¨¡å¼:
Module A -> Module B -> ... -> Module A

ä¸¥é‡ç¨‹åº¦:
- 2-hop å¾ªç¯ (A->B->A): CRITICAL âš ï¸
- 3+ hop å¾ªç¯: HIGH âš ï¸
```

**ç”¨æ³•**:

```python
from app.tasks.architectural_drift import detect_cyclic_dependencies

# å¼‚æ­¥ä»»åŠ¡
task_result = detect_cyclic_dependencies.apply_async(
    args=['project-id'],
    queue='low_priority'
)

# è½®è¯¢ç»“æœ
from celery.result import AsyncResult
result = AsyncResult(task_result.id)
while result.status != 'SUCCESS':
    print(f"Status: {result.status}")
    # ç­‰å¾…...

print(result.result)
```

**ç¤ºä¾‹ç»“æœ**:

```python
{
    'cycles_found': 2,
    'cycles': [
        {
            'module': 'UserService',
            'cycle_path': ['UserService', 'AuthService', 'UserService'],
            'cycle_length': 2,
            'severity': 'CRITICAL',
            'description': 'å¾ªç¯ä¾èµ–: UserService -> AuthService -> UserService'
        },
        {
            'module': 'OrderService',
            'cycle_path': ['OrderService', 'PaymentService', 'InventoryService', 'OrderService'],
            'cycle_length': 3,
            'severity': 'HIGH',
            'description': 'å¾ªç¯ä¾èµ–: OrderService -> PaymentService -> InventoryService -> OrderService'
        }
    ]
}
```

#### 2. æ£€æµ‹å±‚è¿è§„

**æŸ¥è¯¢** (è§ cypher_queries.py):

```cypher
LAYER_VIOLATION_QUERY

æ£€æµ‹æ¶æ„å±‚è¿è§„:
Controller â†’ Service â†’ Repository (æ­£ç¡®çš„æµç¨‹)
Controller â†’ Repository (è¿è§„! è·³è¿‡äº† Service å±‚)

æ ‡å‡†å±‚:
- Controller å±‚: å¤„ç† HTTP è¯·æ±‚
- Service å±‚: ä¸šåŠ¡é€»è¾‘
- Repository å±‚: æ•°æ®è®¿é—®
- Database: æ•°æ®åº“
```

**ç”¨æ³•**:

```python
from app.tasks.architectural_drift import detect_layer_violations

# å¼‚æ­¥ä»»åŠ¡
task_result = detect_layer_violations.apply_async(
    args=['project-id'],
    queue='low_priority'
)

result = AsyncResult(task_result.id)
print(result.result)
```

**ç¤ºä¾‹ç»“æœ**:

```python
{
    'violations_found': 3,
    'violations': [
        {
            'source_module': 'UserController',
            'source_type': 'Controller',
            'target_module': 'UserRepository',
            'target_type': 'Repository',
            'violation_path': ['UserController', 'SomeHelper', 'UserRepository'],
            'violation_type': 'layer_skip',
            'severity': 'HIGH',
            'description': 'Layer violation: UserController (Controller) bypasses Service layer and directly depends on UserRepository (Repository)',
            'recommendation': 'Add intermediate Service layer to maintain proper architecture layers'
        }
    ]
}
```

#### 3. è€¦åˆæŒ‡æ ‡

**æŸ¥è¯¢**:

```cypher
EFFERENT_COUPLING_QUERY: å‡ºå‘è€¦åˆåº¦ (æ¨¡å—ä¾èµ–å¤šå°‘ä¸ªå…¶ä»–æ¨¡å—)
AFFERENT_COUPLING_QUERY: å…¥å‘è€¦åˆåº¦ (å¤šå°‘ä¸ªå…¶ä»–æ¨¡å—ä¾èµ–è¯¥æ¨¡å—)
INSTABILITY_INDEX_QUERY: ä¸ç¨³å®šæ€§æŒ‡æ•° (EC / (EC + AC))

ä¸ç¨³å®šæ€§æŒ‡æ•°è§£é‡Š:
- 0.0-0.3: STABLE (ç¨³å®šæ ¸å¿ƒ - è®¸å¤šæ¨¡å—ä¾èµ–å®ƒ)
- 0.3-0.7: BALANCED (å¹³è¡¡æ¨¡å—)
- 0.7-1.0: UNSTABLE (ä¸ç¨³å®šå¶å­ - ä¾èµ–å¾ˆå¤šå…¶ä»–æ¨¡å—)
```

### å®šæ—¶è°ƒåº¦é…ç½®

åœ¨ `celery_config.py` ä¸­:

```python
beat_schedule={
    # æ¯å‘¨ä¸€ä¸Šåˆ 2 ç‚¹ (UTC)
    'detect-drift-weekly': {
        'task': 'app.tasks.architectural_drift.detect_architectural_drift',
        'schedule': crontab(day_of_week='monday', hour=2, minute=0),
    },

    # æ¯å¤©ä¸Šåˆ 3 ç‚¹ (UTC)
    'detect-cycles-daily': {
        'task': 'app.tasks.architectural_drift.detect_cyclic_dependencies',
        'schedule': crontab(hour=3, minute=0),
    },

    # å‘¨ä¸€å’Œå‘¨å››ä¸Šåˆ 4 ç‚¹ (UTC)
    'detect-violations-twice-weekly': {
        'task': 'app.tasks.architectural_drift.detect_layer_violations',
        'schedule': crontab(day_of_week='monday,thursday', hour=4, minute=0),
    }
}
```

### ä½¿ç”¨ Cron å‘½ä»¤è¡Œ

å¦‚æœä¸ä½¿ç”¨ Celery Beatï¼Œå¯ä»¥ç”¨ç³»ç»Ÿ cron:

```bash
# ç¼–è¾‘ crontab
crontab -e

# æ·»åŠ ä»¥ä¸‹è¡Œ:
# æ¯å‘¨ä¸€ä¸Šåˆ 2 ç‚¹è¿è¡Œæ¼‚ç§»æ£€æµ‹
0 2 * * 1 cd /app && celery -A app.celery_config celery_app call app.tasks.detect_architectural_drift --args='["project-id"]' --queue=low_priority

# æ¯å¤©ä¸Šåˆ 3 ç‚¹è¿è¡Œå¾ªç¯æ£€æµ‹
0 3 * * * cd /app && celery -A app.celery_config celery_app call app.tasks.detect_cyclic_dependencies --args='["project-id"]' --queue=low_priority

# æ¯å‘¨ä¸€å’Œå‘¨å››ä¸Šåˆ 4 ç‚¹è¿è¡Œå±‚è¿è§„æ£€æµ‹
0 4 * * 1,4 cd /app && celery -A app.celery_config celery_app call app.tasks.detect_layer_violations --args='["project-id"]' --queue=low_priority
```

---

## Docker Compose é…ç½®

### å·²æ›´æ–°çš„æœåŠ¡

```yaml
services:
  # ç°æœ‰æœåŠ¡ (backend, postgres, neo4j, redis)

  # NEW: High Priority Celery Worker
  celery-worker-high:
    - å¤„ç† PR åˆ†æä»»åŠ¡
    - å¹¶å‘æ•°: 2 (å¯æ ¹æ®éœ€è¦è°ƒæ•´)
    - é˜Ÿåˆ—: high_priority
    - å‘½ä»¤: celery -A app.celery_config celery_app worker --queues=high_priority

  # NEW: Low Priority Celery Worker
  celery-worker-low:
    - å¤„ç†æ¶æ„æ¼‚ç§»æ£€æµ‹
    - å¹¶å‘æ•°: 1
    - é˜Ÿåˆ—: low_priority,default
    - å‘½ä»¤: celery -A app.celery_config celery_app worker --queues=low_priority,default

  # NEW: Celery Beat Scheduler
  celery-beat:
    - è§¦å‘å®šæ—¶ä»»åŠ¡
    - è°ƒåº¦å‘¨æœŸä»»åŠ¡
    - å‘½ä»¤: celery -A app.celery_config celery_app beat --scheduler django_celery_beat.schedulers:DatabaseScheduler

volumes:
  celery_beat_schedule: # Celery Beat çš„æŒä¹…åŒ–å­˜å‚¨
```

---

## è¿è¡Œä¸éƒ¨ç½²

### æœ¬åœ°å¼€å‘

#### å¯åŠ¨æ‰€æœ‰æœåŠ¡

```bash
docker-compose up -d

# éªŒè¯æœåŠ¡
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f backend
docker-compose logs -f celery-worker-high
docker-compose logs -f celery-worker-low
docker-compose logs -f celery-beat
```

#### æ£€æŸ¥æœåŠ¡å¥åº·

```bash
# åç«¯å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# Redis
redis-cli -h localhost -p 6379 -a <REDIS_PASSWORD> ping
# é¢„æœŸ: PONG

# æ£€æŸ¥ Celery è¿æ¥
celery -A app.celery_config celery_app inspect active
```

### ç”Ÿäº§éƒ¨ç½²

#### ä½¿ç”¨ Kubernetes

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-worker-high
spec:
  replicas: 3 # 3 ä¸ªå‰¯æœ¬å¤„ç† PR åˆ†æ
  template:
    spec:
      containers:
        - name: celery-worker
          image: your-registry/backend:latest
          command:
            [
              "celery",
              "-A",
              "app.celery_config",
              "celery_app",
              "worker",
              "--queues=high_priority",
              "--concurrency=2",
            ]
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: celery-beat
spec:
  replicas: 1 # åªéœ€è¦ä¸€ä¸ª Beat å®ä¾‹
  template:
    spec:
      containers:
        - name: celery-beat
          image: your-registry/backend:latest
          command: ["celery", "-A", "app.celery_config", "celery_app", "beat"]
          resources:
            requests:
              memory: "256Mi"
              cpu: "100m"
```

#### ä½¿ç”¨ Docker Swarm

```bash
docker stack deploy -c docker-compose.yml ai-review
docker service ls
docker service logs ai-review_celery-worker-high
```

---

## ç›‘æ§ä¸è°ƒè¯•

### Celery ç›‘æ§

#### ä½¿ç”¨ Flower (Celery ç›‘æ§å·¥å…·)

```bash
# å®‰è£…
pip install flower

# å¯åŠ¨ Flower
celery -A app.celery_config celery_app flower

# è®¿é—®
http://localhost:5555

# å¯ä»¥çœ‹åˆ°:
- æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
- ä»»åŠ¡å†å²
- Worker çŠ¶æ€
- å®æ—¶ç»Ÿè®¡
```

#### å‘½ä»¤è¡Œæ£€æŸ¥

```bash
# æŸ¥çœ‹æ´»åŠ¨ä»»åŠ¡
celery -A app.celery_config celery_app inspect active

# æŸ¥çœ‹ Worker ç»Ÿè®¡
celery -A app.celery_config celery_app inspect stats

# æŸ¥çœ‹ä»»åŠ¡ç»“æœ
celery -A app.celery_config celery_app inspect result <task_id>

# æŸ¥çœ‹å·²æ³¨å†Œçš„ä»»åŠ¡
celery -A app.celery_config celery_app inspect registered

# æŸ¥çœ‹é˜Ÿåˆ—
celery -A app.celery_config celery_app inspect active_queues
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹ç‰¹å®š Worker çš„æ—¥å¿—
docker-compose logs celery-worker-high -f --tail=100

# æŸ¥çœ‹ç‰¹å®šä»»åŠ¡çš„æ—¥å¿—
docker-compose exec celery-worker-high celery -A app.celery_config celery_app events --dump

# ç­›é€‰é”™è¯¯æ—¥å¿—
docker-compose logs celery-worker-high | grep ERROR
```

### æ€§èƒ½ç›‘æµ‹

```python
# åœ¨åº”ç”¨ä¸­æ·»åŠ åº¦é‡æŒ‡æ ‡
from app.core.metrics import metrics

@celery_app.task(bind=True)
def analyze_pull_request(self, pr_id: str, project_id: str):
    start_time = time.time()

    try:
        # ... ä»»åŠ¡é€»è¾‘ ...

        elapsed = time.time() - start_time
        metrics.histogram(
            'task.analyze_pr.duration_seconds',
            elapsed,
            tags=['pr_id', pr_id, 'status', 'success']
        )
    except Exception as e:
        elapsed = time.time() - start_time
        metrics.histogram(
            'task.analyze_pr.duration_seconds',
            elapsed,
            tags=['pr_id', pr_id, 'status', 'failure']
        )
        raise
```

---

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### é—®é¢˜ 1: ä»»åŠ¡å¡åœ¨ PENDING

**ç—‡çŠ¶**: ä»»åŠ¡é•¿æ—¶é—´åœç•™åœ¨ PENDING çŠ¶æ€

**å¯èƒ½åŸå› **:

1. Worker æœªå¯åŠ¨æˆ–å´©æºƒ
2. Redis è¿æ¥é—®é¢˜
3. ä»»åŠ¡æ²¡æœ‰æ­£ç¡®è·¯ç”±åˆ°é˜Ÿåˆ—

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ Worker çŠ¶æ€
celery -A app.celery_config celery_app inspect active

# é‡å¯ Worker
docker-compose restart celery-worker-high

# æ£€æŸ¥ Redis è¿æ¥
redis-cli -h localhost -p 6379 -a <PASSWORD> ping

# æŸ¥çœ‹ä»»åŠ¡åœ¨å“ªä¸ªé˜Ÿåˆ—
docker-compose exec redis redis-cli -a <PASSWORD> KEYS "celery*"
```

#### é—®é¢˜ 2: Worker é¢‘ç¹é‡å¯

**ç—‡çŠ¶**: Worker å®¹å™¨ä¸€ç›´é‡å¯

**å¯èƒ½åŸå› **:

1. å†…å­˜ä¸è¶³
2. ä»»åŠ¡å¯¼è‡´åˆ†æ®µé”™è¯¯
3. ä¾èµ–é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æŸ¥çœ‹æ—¥å¿—
docker-compose logs celery-worker-high --tail=50

# å¢åŠ å†…å­˜é™åˆ¶ (docker-compose.yml)
celery-worker-high:
  deploy:
    resources:
      limits:
        memory: 2G

# é‡å¯
docker-compose restart celery-worker-high
```

#### é—®é¢˜ 3: ä»»åŠ¡è¶…æ—¶

**ç—‡çŠ¶**: ä»»åŠ¡åœ¨å®Œæˆå‰è¶…æ—¶

**å¯èƒ½åŸå› **:

1. ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿
2. Neo4j æŸ¥è¯¢æ•ˆç‡ä½
3. ç½‘ç»œå»¶è¿Ÿ

**è§£å†³æ–¹æ¡ˆ**:

```python
# åœ¨ celery_config.py ä¸­å¢åŠ è¶…æ—¶
celery_app.conf.update(
    task_soft_time_limit=600,  # 10 åˆ†é’Ÿè½¯é™åˆ¶
    task_time_limit=900,       # 15 åˆ†é’Ÿç¡¬é™åˆ¶
)

# æˆ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡
@celery_app.task(
    time_limit=600,
    soft_time_limit=500
)
def analyze_pull_request(self, pr_id: str, project_id: str):
    pass
```

#### é—®é¢˜ 4: Redis è¿æ¥æ‹’ç»

**ç—‡çŠ¶**: `ConnectionRefusedError: Error connecting to Redis`

**å¯èƒ½åŸå› **:

1. Redis å®¹å™¨æœªè¿è¡Œ
2. å¯†ç ä¸æ­£ç¡®
3. ç«¯å£è¢«å ç”¨

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ Redis çŠ¶æ€
docker-compose ps redis

# æ£€æŸ¥ç«¯å£
lsof -i :6379

# é‡å¯ Redis
docker-compose restart redis

# éªŒè¯è¿æ¥
docker-compose exec redis redis-cli -a <PASSWORD> ping
```

### è°ƒè¯•æ¨¡å¼

```python
# å¯ç”¨è°ƒè¯•æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨ Celery è°ƒè¯•
celery -A app.celery_config celery_app worker --loglevel=DEBUG

# åœ¨ä»£ç ä¸­æ·»åŠ è°ƒè¯•
@celery_app.task(bind=True)
def analyze_pull_request(self, pr_id: str, project_id: str):
    print(f"DEBUG: Starting task {self.request.id}")
    print(f"DEBUG: PR ID: {pr_id}, Project ID: {project_id}")
    # ...
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è°ƒæ•´ Worker å¹¶å‘æ•°

```bash
# PR åˆ†æ Worker: å¤„ç† I/O å¯†é›†ä»»åŠ¡ï¼Œå¯ä»¥å¢åŠ å¹¶å‘
celery worker --queues=high_priority --concurrency=4 --pool=prefork

# æ¶æ„æ£€æµ‹ Worker: å¤„ç†è®¡ç®—å¯†é›†ä»»åŠ¡ï¼Œä¿æŒè¾ƒä½å¹¶å‘
celery worker --queues=low_priority --concurrency=1
```

### 2. ä¼˜åŒ– Cypher æŸ¥è¯¢

```cypher
// æ·»åŠ ç´¢å¼•åŠ å¿«æŸ¥è¯¢
CREATE INDEX project_id ON :Project(projectId);
CREATE INDEX module_id ON :Module(moduleId);
CREATE INDEX depends_on ON :DEPENDS_ON(source, target);

// ä½¿ç”¨ EXPLAIN åˆ†ææŸ¥è¯¢è®¡åˆ’
EXPLAIN MATCH (m:Module)-[:DEPENDS_ON*]->(m) RETURN m
```

### 3. å®ç°ç»“æœç¼“å­˜

```python
# åœ¨ analyze_pull_request ä¸­
@celery_app.task
def analyze_pull_request(self, pr_id: str, project_id: str):
    # æ£€æŸ¥ç¼“å­˜
    cache_key = f"pr_analysis:{pr_id}"
    cached_result = await cache.get(cache_key)
    if cached_result:
        return cached_result

    # ... æ‰§è¡Œåˆ†æ ...

    # ç¼“å­˜ç»“æœ 1 å°æ—¶
    await cache.set(cache_key, result, expires=3600)
    return result
```

### 4. æ‰¹é‡å¤„ç†ä»»åŠ¡

```python
# ä¸è¦è¿™æ ·åš - å¤ªå¤šå°ä»»åŠ¡
for pr_id in pr_list:
    analyze_pull_request.apply_async([pr_id, project_id])

# è¿™æ ·åš - æ‰¹å¤„ç†
from celery import group
job = group(
    analyze_pull_request.s(pr_id, project_id)
    for pr_id in pr_list
)
result = job.apply_async()
```

---

## æ€»ç»“

æœ¬æŒ‡å—æ¶µç›–äº†:
âœ… å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—è®¾ç½®ä¸ä½¿ç”¨
âœ… æ¶æ„æ¼‚ç§»æ£€æµ‹æŸ¥è¯¢
âœ… Docker Compose é…ç½®
âœ… è¿è¡Œä¸éƒ¨ç½²æŒ‡å—
âœ… ç›‘æ§ä¸è°ƒè¯•å·¥å…·
âœ… æ•…éšœæ’é™¤ä¸æ€§èƒ½ä¼˜åŒ–

å®Œæ•´çš„å®ç°åŒ…æ‹¬:

- 6 ä¸ª Celery ä»»åŠ¡æ–‡ä»¶
- 8+ Cypher æŸ¥è¯¢æ¨¡æ¿
- å®Œæ•´çš„æµ‹è¯•å¥—ä»¶
- Docker Compose é…ç½®
- API ç«¯ç‚¹
- å®šæ—¶è°ƒåº¦é…ç½®

æ‰€æœ‰ä»£ç å·²å‡†å¤‡å¥½ç”Ÿäº§éƒ¨ç½²! ğŸš€
